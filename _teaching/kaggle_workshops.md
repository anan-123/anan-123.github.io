---
title: "Kaggle Workshops"
collection: teaching
type: "Workshops"
venue: https://www.kaggle.com/
permalink: /teaching/kaggle_workshops
date: 2019/12/1
---

These are a collection of all the workshops I've run at Kaggle, from July 2016 to December 2019.

Practical Model Evaluation (with AutoML)
----------------------------------------

How do you know which machine learning model  is going to work best for a specific problem? Learning how to evaluate machine learning models is an important part of the data science workflow. You'll need it for everything from picking your final submissions for a Kaggle competition to choosing which model your team should put into production.

We know how important model evaluation is, so we've put together a three-day workshop to walk you through the model evaluation process from start to finish. We'll go beyond just optimization metrics, though, and project about factors for model selection relevant to working data scientists.

-   Day 1 Notebook: [Figuring out what matters for you](https://www.kaggle.com/rtatman/practical-model-evaluation-day-1)

-   Day 2 Notebook: [Training models with automated machine learning](https://www.kaggle.com/rtatman/practical-model-evaluation-day-2)

-   Day 3 Notebook: [Evaluating our models](https://www.kaggle.com/rtatman/practical-model-evaluation-day-3)

Utility Script Competition
--------------------------

As part of the [Utility Script Competition](https://www.kaggle.com/general/109651) we ran on Kaggle, I wrote [this notebook](https://www.kaggle.com/rtatman/six-steps-to-more-professional-data-science-code) with guidelines for writing more professional data science code.

SQL Summer Camp
---------------

As part of the SQL Summer Camp I wrote a workshop as an introduction to BigQuery ML. It is based on the [official documentation tutorial](https://cloud.google.com/bigquery/docs/bigqueryml-scientist-start). In this tutorial, you use the sample [Google Analytics sample dataset for BigQuery](https://support.google.com/analytics/answer/7586738?hl=en&ref_topic=3416089) to create a model that predicts whether a website visitor will make a transaction. For information on the schema of the Analytics dataset, see [BigQuery export schema](https://support.google.com/analytics/answer/3437719). 

-   [BigQuery Machine Learning Tutorial ](https://www.kaggle.com/rtatman/bigquery-machine-learning-tutorial)

-   [Exercises](https://www.kaggle.com/rtatman/bigquery-machine-learning-exercise)

-   [Exercises with Solutions](https://www.kaggle.com/rtatman/bigquery-machine-learning-exercise-w-solutions)

Intro to API's
--------------

This was a three day event held during Kaggle CareerCon 2019. Each day we learned about a new part of developing an API and put it into practice. By day 3, you'll have written and deployed an API of your very own!

-   [Day 1: The Basics of Rest APIs -- What They Are and How to Design One.](https://www.kaggle.com/rtatman/careercon-intro-to-apis) By the end of this day you'll have written the OpenAPI specification for your API.

-   [Day 2: How to Make an API for an Existing Python Machine Learning Project.](https://www.kaggle.com/rtatman/careercon-making-an-app-from-your-modeling-code) By the end of this day, you'll have a Flask app that you can use to serve your model.

-   [Day 3: How to deploy your API on your choice of services -- Heroku or Google Cloud.](https://www.kaggle.com/rtatman/careercon-deploying-apis-on-heroku-appengine/) By the end of this day, you'll have deployed your model and will be able to actually use your API! (Note that, in order to use Google Cloud, you'll need to create a billing account, which requires a credit card. If you don't have access to a credit, you can still use Heroku.)

Getting Started with Automated Data Pipelines
---------------------------------------------

The Getting Started with Automated Data Pipelines series is a set of three notebooks and livestreams (recordings are available) designed to help you get started with creating data pipeline that allow you to automate the process of moving and transforming data.

-   Day 1: Versioning & Creating Datasets from GitHub Repos

-   [Notebook](https://www.kaggle.com/rtatman/kerneld4769833fe/)

-   [Livestream](https://youtu.be/Xi140XVOznM)

-   Day 2: Validation & Creating Datasets from URL's

-   [Notebook](https://www.kaggle.com/rtatman/automating-data-pipelines-day-2)

-   [Livestream](https://youtu.be/-wF1hSEQqIc)

-   Day 3: ETL & Creating Datasets from Kernel Output

-   [Notebook](https://www.kaggle.com/rtatman/automating-data-pipelines-day-3)

-   [Livestream](https://youtu.be/2pWifnSPN5E)

Dashboarding with Notebooks
---------------------------

![](https://lh5.googleusercontent.com/kVxfPbnin37yk9yR-Fslf8gw9YytHFU5hxeKz_PjEzwEdVvcm2CnjuNx7AGjXA0NL0g3ZMjljOYnpBl9S0c-P6bVYW7-FoIxn6DInZmXg76qZPEm59A9lC4nKHkfhIHqcv3Upf4U)

Want to learn how to combine the speed of spinning up a notebook with the ease of an automatically updating dashboard? Then this is the event for you! Each day from December 17th to December 21st 2018 you'll get a practical, hands-on exercise that won't take more than 20 minutes but will help you refine your dashboarding skills.

-   Day 1: Determining what information should be monitored with a dashboard. [Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-1), [Livestream Recording](https://www.youtube.com/watch?v=QO2ihJS2QLM)

-   Day 2: How to create effective dashboards in notebooks, [Python Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-2-python), [R Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-2-r), [Livestream](https://www.youtube.com/watch?v=rhi_nexCUMI)

-   Day 3: Running notebooks with the Kaggle API, [Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-3), [Livestream](https://youtu.be/cdEUEe2scNo)

-   Day 4: Scheduling notebook runs using cloud services, [Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-4), [Livestream](https://youtu.be/Oujj6nT7etY)

-   Day 5: Testing and validation, [Python Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-5), [R Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-5-r), [Livestream](https://www.youtube.com/watch?v=H6DcpIykT8E)

JupyterCon 2018 Workshops
-------------------------

At JupyterCon 2018 I gave two workshops. You can find all my materials in these two notebooks:

-   [Reproducible research best practices (highlighting Kaggle Kernels)](https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68323) 

-   [I Do, We Do, You Do: Supporting active learning with notebooks](https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68277)

5-Day Challenges
================

I ran several educational 5-Day Challenges on different topics in 2017 & 2018. Each challenge consists of five short exercises designed to give you hands-on practice with a different data science technique. This notebook collects links to the exercises for each challenge so you can work through them at your own pace.

* * * * *

[5-Day Data Challenge](https://www.kaggle.com/rtatman/the-5-day-data-challenge)
-------------------------------------------------------------------------------

-   Topic: Getting started with data science

-   Level: Beginner

-   Language: Python and R

-   Daily tasks:

-   Day 1: Reading data into a kernel

-   Day 2: Plot a Numeric Variable with a Histogram

-   Day 3: Perform a t-test

-   Day 4: Visualize categorical data with a bar chart

-   Day 5: Using a Chi-Square Test

New to data science? Need a quick refresher? This five day challenge will give you the guidance and support you need to kick-start your data science journey.

By the time you finish this challenge, you will:

-   Read in and summarize data

-   Visualize both numeric and categorical data

-   Know when and how to use two foundational statistical tests (t-test and chi-squared)

All the material for this challenge [is in one notebook](https://www.kaggle.com/rtatman/the-5-day-data-challenge).

* * * * *

[5-Day Data Challenge: Regression](https://www.kaggle.com/rtatman/the-5-day-regression-challenge)
-------------------------------------------------------------------------------------------------

-   Topic: Regression

-   Level: Intermediate (should already be familiar with R)

-   Language: R

-   Daily tasks:

-   [Day 1: Learn about the different types of regression (Poisson, linear and logistic) and when to use them](https://www.kaggle.com/rtatman/regression-challenge-day-1)

-   [Day 2: Learn how to fit & evaluate a model with diagnostic plots](https://www.kaggle.com/rtatman/regression-challenge-day-2)

-   [Day 3: Learn how to read and understand models](https://www.kaggle.com/rtatman/regression-challenge-day-3)

-   [Day 4: Learn how to fit & interpret a multiple regression model](https://www.kaggle.com/rtatman/regression-challenge-day-4)

-   [Day 5: Learn how to use Elastic Net to select input variables](https://www.kaggle.com/rtatman/regression-challenge-day-5)

By the time you finish this challenge, you'll understand how and when to implement three foundational regression techniques. Each day we will cover one aspect of regression analysis in depth.

-   How to pick the right regression technique for your data

-   How to use diagnostic plots to check your model

-   How to interpret and communicate your model

-   Visualizing your model

-   Comparing models & selecting variables

We'll work with real datasets to help develop an intuitive understanding of how each type of model works and how to interpret the results.

* * * * *

[SQL Scavenger Hunt](https://www.kaggle.com/rtatman/sql-scavenger-hunt-handbook/) (not a 5-Day Challenge, but follows a similar format)
---------------------------------------------------------------------------------------------------------------------------------------

-   Topic: How to query data in SQL

-   Level: Beginner

-   Language: Python and SQL

-   Daily tasks:

-   [Before you start: How to use SQL on Kaggle](https://www.kaggle.com/rtatman/sql-scavenger-hunt-handbook/)

-   [Day 1: SELECT FROM](https://www.kaggle.com/rtatman/sql-scavenger-hunt-day-1/)

-   [Day 2: GROUP BY](https://www.kaggle.com/rtatman/sql-scavenger-hunt-day-2/)

-   [Day 3: ORDER BY & Dates](https://www.kaggle.com/rtatman/sql-scavenger-hunt-day-3/)

-   [Day 4: WITH & AS](https://www.kaggle.com/rtatman/sql-scavenger-hunt-day-4/)

-   [Day 5: JOIN](https://www.kaggle.com/rtatman/sql-scavenger-hunt-day-5/)

In our SQL Scavenger Hunt, you'll learn how to use SQL to get data from BigQuery databases. Each day you'll learn about a core SQL technique and practice using it to get the data you need to answer real-world questions like:

-   How many GitHub users made more than ten commits on January 1, 2015?

-   Which five cities had the highest air pollution last week?

-   You'll also learn best practices for working with BIG datasets.

SQL (short for "Structured Query Language") is the primary way to get data out of relational databases. It's also the third most popular software tool for data science, right after Python and R, and a key skill for aspiring data scientists to develop.

### [This challenge is also available as a Learn track](https://www.kaggle.com/learn/sql)

* * * * *

Python: [5-Day Data Challenge: Data Cleaning](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values)
-----------------------------------------------------------------------------------------------------------------------------

-   Topic: Data cleaning

-   Level: Beginner to intermediate (should already be familiar with Python)

-   Language: Python

-   Daily tasks:

-   [Day 1: Handling missing values](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values)

-   [Day 2: Scaling and normalization](https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data)

-   [Day 3: Parsing dates](https://www.kaggle.com/rtatman/data-cleaning-challenge-parsing-dates/)

-   [Day 4: Character encodings](https://www.kaggle.com/rtatman/data-cleaning-challenge-character-encodings/)

-   [Day 5: Inconsistent Data Entry](https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry/)

Data cleaning is a key part of data science, but it can be deeply frustrating. Why are some of your text fields garbled? What should you do about those missing values? Why aren't your dates formatted correctly? How can you quickly clean up inconsistent data entry? In this five day challenge, you'll learn why you've run into these problems and, more importantly, how to fix them!

In this challenge we'll learn how to tackle some of the most common data cleaning problems so you can get to actually analyzing your data faster. We'll work through five hands-on exercises with real, messy data and answer some of your most commonly-asked data cleaning questions.

* * * * *

R: [5-Day Data Challenge: Data Cleaning](https://www.kaggle.com/rtatman/data-cleaning-challenge-json-txt-and-xls/)
------------------------------------------------------------------------------------------------------------------

-   Topic: Data cleaning

-   Level: Beginner to intermediate (should already be familiar with R)

-   Language: R

-   Daily tasks:

-   [Day 1: Reading in common data file formats: .json, .txt and .xlsx](https://www.kaggle.com/rtatman/data-cleaning-challenge-json-txt-and-xls/)

-   [Day 2: Filling in missing values](https://www.kaggle.com/rtatman/data-cleaning-challenge-imputing-missing-values/)

-   [Day 3: Identifying & handling outliers](https://www.kaggle.com/rtatman/data-cleaning-challenge-outliers/)

-   [Day 4: Removing duplicate records](https://www.kaggle.com/rtatman/data-cleaning-challenge-deduplication/)

-   [Day 5: Cleaning numbers (percentages, money, dates and times)](https://www.kaggle.com/rtatman/data-cleaning-challenge-cleaning-numeric-columns/)

Data cleaning is a necessary part of data science, but it can be deeply frustrating. What are you supposed to do with this .json file? How can you handle all these missing values in your data? Is there a fast way to get rid of duplicate entries? In this challenge, we'll learn how to solve some common data cleaning problems.

This challenge is in R and covers different topics from the earlier Python version of the Data Cleaning 5-Day Challenge so even if you did the last challenge, you'll discover some new tips and tricks! Here's what we'll be covering:
